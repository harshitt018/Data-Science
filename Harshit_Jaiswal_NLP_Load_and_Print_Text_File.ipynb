{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIW5DiNM-Tnq"
      },
      "outputs": [],
      "source": [
        "#Open the text file, specifying the encoding\n",
        "text_file = open(\"/content/“Twin Stars Aloft” by Charles Kingsley.txt\", encoding=\"latin-1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the data\n",
        "text = text_file.read()"
      ],
      "metadata": {
        "id": "vE3eVK6y_C3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Datatype of the data read :\n",
        "print (type(text))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYuiloX0_CtS",
        "outputId": "752f8432-47b9-487c-f291-b92679bd9273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the text :\n",
        "print(text)\n",
        "print(\"\\n\")\n",
        "#Length of the text :\n",
        "print (len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AJy8t-1_YKD",
        "outputId": "1b652c84-d5fc-4afd-e2a9-953f545e33b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twin stars, aloft in ether clear,\n",
            "Around each other roll alway,\n",
            "Within one common atmosphere\n",
            "Of their own mutual light and day.\n",
            "\n",
            "And myriad happy eyes are bent\n",
            "Upon their changeless love alway;\n",
            "As strengthened by their one intent,\n",
            "They pour the flood of life and day.\n",
            "\n",
            "So we, through this worlds waning night,\n",
            "Shall, hand in hand, pursue our way;\n",
            "Shed round us order, love, and light,\n",
            "And shine unto the perfect day.\n",
            "\n",
            "\n",
            "417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import required libraries :\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf_a1kEJ_ber",
        "outputId": "881c9796-d014-4a42-b790-f6b827903c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the text by sentences :\n",
        "sentences = sent_tokenize(text)\n",
        "#How many sentences are there? :\n",
        "print (len(sentences))\n",
        "#Print the sentences :\n",
        "#print(sentences)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-axLNki_eCb",
        "outputId": "3fd9956d-d1b8-4b32-fe23-8b0d9ef32111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Twin stars, aloft in ether clear,\\nAround each other roll alway,\\nWithin one common atmosphere\\nOf their own mutual light and day.',\n",
              " 'And myriad happy eyes are bent\\nUpon their changeless love alway;\\nAs strengthened by their one intent,\\nThey pour the flood of life and day.',\n",
              " 'So we, through this world\\x92s waning night,\\nShall, hand in hand, pursue our way;\\nShed round us order, love, and light,\\nAnd shine unto the perfect day.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the text with words :\n",
        "words = word_tokenize(text)\n",
        "#How many words are there? :\n",
        "print (len(words))\n",
        "print(\"\\n\")\n",
        "#Print words :\n",
        "print (words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTRuW5BJ_iXS",
        "outputId": "6bb817a0-41e7-40d9-c7a9-4f1ad397d314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n",
            "\n",
            "\n",
            "['Twin', 'stars', ',', 'aloft', 'in', 'ether', 'clear', ',', 'Around', 'each', 'other', 'roll', 'alway', ',', 'Within', 'one', 'common', 'atmosphere', 'Of', 'their', 'own', 'mutual', 'light', 'and', 'day', '.', 'And', 'myriad', 'happy', 'eyes', 'are', 'bent', 'Upon', 'their', 'changeless', 'love', 'alway', ';', 'As', 'strengthened', 'by', 'their', 'one', 'intent', ',', 'They', 'pour', 'the', 'flood', 'of', 'life', 'and', 'day', '.', 'So', 'we', ',', 'through', 'this', 'world\\x92s', 'waning', 'night', ',', 'Shall', ',', 'hand', 'in', 'hand', ',', 'pursue', 'our', 'way', ';', 'Shed', 'round', 'us', 'order', ',', 'love', ',', 'and', 'light', ',', 'And', 'shine', 'unto', 'the', 'perfect', 'day', '.']\n"
          ]
        }
      ]
    }
  ]
}